Automatically generated by Mendeley Desktop 1.14
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Sutton1998,
author = {Sutton, Richard S and Barto, Andrew G},
howpublished = {Hardcover},
isbn = {0262193981},
keywords = {learning,markov-decision-processes,reinforcement-learning},
publisher = {A Bradford Book},
title = {{Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning)}},
url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0262193981},
year = {1998}
}
@article{Ijspeert2002,
abstract = {Presents an approach to movement planning, on-line trajectory
modification, and imitation learning by representing movement plans
based on a set of nonlinear differential equations with well-defined
attractor dynamics. The resultant movement plan remains an autonomous
set of nonlinear differential equations that forms a control policy (CP)
which is robust to strong external perturbations and that can be
modified on-line by additional perceptual variables. We evaluate the
system with a humanoid robot simulation and an actual humanoid robot.
Experiments are presented for the imitation of three types of movements:
reaching movements with one arm, drawing movements of 2-D patterns, and
tennis swings. Our results demonstrate (a) that multi-joint human
movements can be encoded successfully by the CPs, (b) that a learned
movement policy can readily be reused to produce robust trajectories
towards different targets, (c) that a policy fitted for one particular
target provides a good predictor of human reaching movements towards
neighboring targets, and (d) that the parameter space which encodes a
policy is suitable for measuring to which extent two trajectories are
qualitatively similar},
author = {Ijspeert, A.J. and Nakanishi, J. and Schaal, S.},
doi = {10.1109/ROBOT.2002.1014739},
isbn = {0-7803-7272-7},
issn = {<null>},
journal = {Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)},
title = {{Movement imitation with nonlinear dynamical systems in humanoid
robots}},
volume = {2},
year = {2002}
}
@inproceedings{Riedmiller1992,
abstract = {In this paper, a new learning algorithm, RPROP, is proposed. To overcome the inherentdisadvantages of the pure gradient-descent technique of the original backpropagationprocedure, RPROP performs an adaptation of the weight update-values according to thebehaviour of the errorfunction. The results of RPROP on several learning tasks are shownin comparison to other well-known adaptive learning algorithms.1 IntroductionBackpropagation is the most widely used algorithm for supervised learning with multilayeredfeed-forward networks. The basic idea of the backpropagation learning algorithmis the repeated application of the chain rule to compute the influence of each weight inthe network with respect to an arbitrary errorfunction E [1]:@E@w ij=@E@a i@a i@net i@net i@w ij(1)where w ij is the weight from neuron j to neuron i, a i is the activation value and net iis the weighted sum of the inputs of neuron i. Once the partial derivative for each weightis known, the a...},
author = {Riedmiller, Martin and Braun, Heinrich},
booktitle = {Proceedings of the International Symposium on Computer and Information Science VII},
title = {{RPROP - A Fast Adaptive Learning Algorithm}},
url = {http://europepmc.org/abstract/CIT/139504},
year = {1992}
}
@incollection{Antos2008,
abstract = {We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by some policy. We study a variant of fitted Q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action-values. We provide a rigorous analysis of this algorithm, proving what we believe is the first finite-time bound for value-function based algorithms for continuous state and action problems.},
author = {Antos, Andr\'{a}s and Munos, R\'{e}mi and Szepesvari, Csaba},
isbn = {160560352X},
keywords = {Learning/Statistics \& Optimisation,Theory \& Algorithms},
title = {{Fitted Q-iteration in continuous action-space MDPs}},
url = {http://eprints.pascal-network.org/archive/00003804/},
year = {2008}
}
@article{Rougier2011a,
author = {Rougier, Nicolas and Boniface, Yann},
doi = {10.1016/j.neucom.2010.06.034},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Cortical plasticity,Dynamic,On-line,Self-organisation},
month = may,
number = {11},
pages = {1840--1847},
publisher = {Elsevier},
title = {{Dynamic self-organising map}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231211000713},
volume = {74},
year = {2011}
}
@article{Vanderbei1999,
abstract = {The paper describes an interior-point algorithm for nonconvex nonlinear programming which is a direct extension of interior-point methods for linear and quadratic programming. Major modifications include a merit function and an altered search direction to ensure that a descent direction for the merit function is obtained. Preliminary numerical testing indicates that the method is robust. Further, numerical comparisons with minos and lancelot show that the method is efficient, and has the promise of greatly reducing solution times on at least some classes of models.},
author = {Vanderbei, RJ and Shanno, DF},
doi = {10.1023/A:1008677427361},
issn = {09266003},
journal = {Computational Optimization and Applications},
keywords = {and phrases,author supported by afosr,author supported by nsf,grant ccr-9403789 and by,grant f49620-95-1-0110,interior-point methods,nonconvex optimization,nonlinear programming,onr grant n00014-98-1-0036,research of the first,research of the second},
pages = {1--20},
title = {{An interior-point algorithm for nonconvex nonlinear programming}},
url = {http://link.springer.com/article/10.1023/A:1008677427361},
volume = {13},
year = {1999}
}
@article{Grondman2012,
abstract = {Policy-gradient-based actor-critic algorithms are amongst the most popular algorithms in the reinforcement learning framework. Their advantage of being able to search for optimal policies using low-variance gradient estimates has made them useful in several real-life applications, such as robotics, power control, and finance. Although general surveys on reinforcement learning techniques already exist, no survey is specifically dedicated to actor-critic algorithms in particular. This paper, therefore, describes the state of the art of actor-critic algorithms, with a focus on methods that can work in an online setting and use function approximation in order to deal with continuous state and action spaces. After starting with a discussion on the concepts of reinforcement learning and the origins of actor-critic algorithms, this paper describes the workings of the natural gradient, which has made its way into many actor-critic algorithms over the past few years. A review of several standard and natural actor-critic algorithms is given, and the paper concludes with an overview of application areas and a discussion on open issues.},
author = {Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel a D and Babu\v{s}ka, Robert},
doi = {10.1109/TSMCC.2012.2218595},
file = {:home/mzimmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grondman et al. - 2012 - A survey of actor-critic reinforcement learning Standard and natural policy gradients.pdf:pdf},
issn = {10946977},
journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
keywords = {Actor-critic,natural gradient,policy gradient,reinforcement learning (RL)},
number = {6},
pages = {1291--1307},
title = {{A survey of actor-critic reinforcement learning: Standard and natural policy gradients}},
volume = {42},
year = {2012}
}
@article{Kingma2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
file = {:home/mzimmer/cours/Doctorat/Papers/1412.6980v8.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@article{Dutech2012,
author = {Dutech, Alain},
file = {:home/mzimmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dutech - 2012 - Self-organizing developmental reinforcement learning.pdf:pdf},
journal = {From Animals to Animats 12},
pages = {1--11},
title = {{Self-organizing developmental reinforcement learning}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33093-3\_31},
year = {2012}
}
@book{Howard1960,
address = {Cambridge, MA},
author = {Howard, RA},
publisher = {MIT Press},
title = {{Dynamic Programming and Markov Processes}},
url = {http://trid.trb.org/view.aspx?id=521429},
year = {1960}
}
@article{Oudeyer2007,
abstract = {Exploratory activities seem to be intrinsically rewarding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Experimental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology},
author = {Oudeyer, Pierre Yves and Kaplan, Fr\'{e}d\'{e}ric and Hafner, Verena V.},
doi = {10.1109/TEVC.2006.890271},
isbn = {1089-778X},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Active learning,Autonomy,Behavior,Complexity,Curiosity,Development,Developmental trajectory,Epigenetic robotics,Intrinsic motivation,Learning,Reinforcement learning,Values},
number = {2},
pages = {265--286},
pmid = {3239607},
title = {{Intrinsic motivation systems for autonomous mental development}},
volume = {11},
year = {2007}
}
@inproceedings{Igel2000,
abstract = {The Rprop algorithm proposed by Riedmiller and Braun is one of the best performing first-order learning methods for neural networks. We intro- duce modifications of the algorithm that improve its learning speed. The resulting speedup is experi- mentally shown for a set of neural network learning tasks as well as for artificial error surfaces.},
author = {Igel, Christian and H\"{u}sken, M},
booktitle = {Proceedings of the Second International Symposium on Neural Computation},
keywords = {individual,network training is weight-backtracking,rprop,step-size adaptation,supervised learning,weight-,weight-backtracking},
pages = {115--121},
title = {{Improving the Rprop learning algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.3899\&rep=rep1\&type=pdf},
year = {2000}
}
@article{Baranes2013,
author = {Baranes, Adrien and Oudeyer, Pierre-Yves},
doi = {10.1016/j.robot.2012.05.008},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
month = jan,
number = {1},
pages = {49--73},
publisher = {Elsevier B.V.},
title = {{Active learning of inverse models with intrinsically motivated goal exploration in robots}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889012000644},
volume = {61},
year = {2013}
}
@inproceedings{Melo2008,
abstract = {n this paper we address reinforcement learning problems with continuous state-action spaces. We propose a new algorithm, fitted natural actor-critic (FNAC), that extends the work in [1] to allow for general function approximation and data reuse. We combine the natural actor-critic architecture [1] with a variant of fitted value iteration using importance sampling. The method thus obtained combines the appealing features of both approaches while overcoming their main weaknesses: the use of a gradient-based actor readily overcomes the difficulties found in regression methods with policy optimization in continuous action-spaces; in turn, the use of a regression-based critic allows for efficient use of data and avoids convergence problems that TD-based critics often exhibit. We establish the convergence of our algorithm and illustrate its application in a simple continuous space, continuous action problem.},
author = {Melo, Francisco S. and Lopes, Manuel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87481-2\_5},
isbn = {3540874801},
issn = {03029743},
number = {PART 2},
pages = {66--81},
title = {{Fitted natural actor-critic: A new algorithm for continuous state-action MDPs}},
volume = {5212 LNAI},
year = {2008}
}
@inproceedings{Melo2008a,
abstract = {n this paper we address reinforcement learning problems with continuous state-action spaces. We propose a new algorithm, fitted natural actor-critic (FNAC), that extends the work in [1] to allow for general function approximation and data reuse. We combine the natural actor-critic architecture [1] with a variant of fitted value iteration using importance sampling. The method thus obtained combines the appealing features of both approaches while overcoming their main weaknesses: the use of a gradient-based actor readily overcomes the difficulties found in regression methods with policy optimization in continuous action-spaces; in turn, the use of a regression-based critic allows for efficient use of data and avoids convergence problems that TD-based critics often exhibit. We establish the convergence of our algorithm and illustrate its application in a simple continuous space, continuous action problem.},
author = {Melo, Francisco S. and Lopes, Manuel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87481-2\_5},
isbn = {3540874801},
issn = {03029743},
number = {PART 2},
pages = {66--81},
title = {{Fitted natural actor-critic: A new algorithm for continuous state-action MDPs}},
volume = {5212 LNAI},
year = {2008}
}
@misc{Tesauro1994,
abstract = {TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results, based on the TD($\lambda$) reinforcement learning algorithm (Sutton 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e., given only a raw description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set of hand-crafted features is added to the network's input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players.},
author = {Tesauro, Gerald},
booktitle = {Neural Computation},
doi = {10.1162/neco.1994.6.2.215},
isbn = {0899-7667},
issn = {0899-7667},
number = {2},
pages = {215--219},
title = {{TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play}},
volume = {6},
year = {1994}
}
@article{Geijtenbeek2013,
abstract = {We present a muscle-based control method for simulated bipeds in which both the muscle routing and control parameters are optimized. This yields a generic locomotion control method that supports a variety of bipedal creatures. All actuation forces are the result of 3D simulated muscles, and a model of neural delay is included for all feedback paths. As a result, our controllers generate torque patterns that incorporate biomechanical constraints. The synthesized controllers find different gaits based on target speed, can cope with uneven terrain and external perturbations, and can steer to target directions.},
author = {Geijtenbeek, Thomas and van de Panne, Michiel and van der Stappen, a. Frank},
doi = {10.1145/2508363.2508399},
file = {:home/mzimmer/articles/2013-TOG-MuscleBasedBipeds.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {musculoskeletal simulatio,musculoskeletal simulation,physics-based animation},
pages = {1--11},
title = {{Flexible muscle-based locomotion for bipedal creatures}},
url = {http://dl.acm.org/citation.cfm?doid=2508363.2508399},
volume = {32},
year = {2013}
}
@article{Laflaqui2013,
author = {Laflaqui, Alban},
file = {:home/mzimmer/articles/Thesis.pdf:pdf},
title = {{Approche sensorimotrice de la perception de lâ€™espace pour la robotique autonome}},
year = {2013}
}
@inproceedings{Farahmand2009,
abstract = {Reinforcement learning with linear and non-linear function approximation has been studied extensively in the last decade. However, as opposed to other fields of machine learning such as supervised learning, the effect of finite sample has not been thoroughly addressed within the reinforcement learning framework. In this paper we propose to use L<sup>2</sup> regularization to control the complexity of the value function in reinforcement learning and planning problems. We consider the Regularized Fitted Q-Iteration algorithm and provide generalization bounds that account for small sample sizes. Finally, a realistic visual-servoing problem is used to illustrate the benefits of using the regularization procedure.},
author = {Farahmand, Amir Massoud and Ghavamzadeh, Mohammad and Szepesv\'{a}ri, Csaba and Mannor, Shie},
booktitle = {Proceedings of the American Control Conference},
doi = {10.1109/ACC.2009.5160611},
isbn = {9781424445240},
issn = {07431619},
pages = {725--730},
title = {{Regularized fitted q-iteration for planning in continuous-space markovian decision problems}},
year = {2009}
}
@article{Fremaux2013,
abstract = {Animals repeat rewarded behaviors, but the physiological basis of reward-based learning has only been partially elucidated. On one hand, experimental evidence shows that the neuromodulator dopamine carries information about rewards and affects synaptic plasticity. On the other hand, the theory of reinforcement learning provides a framework for reward-based learning. Recent models of reward-modulated spike-timing-dependent plasticity have made first steps towards bridging the gap between the two approaches, but faced two problems. First, reinforcement learning is typically formulated in a discrete framework, ill-adapted to the description of natural situations. Second, biologically plausible models of reward-modulated spike-timing-dependent plasticity require precise calculation of the reward prediction error, yet it remains to be shown how this can be computed by neurons. Here we propose a solution to these problems by extending the continuous temporal difference (TD) learning of Doya (2000) to the case of spiking neurons in an actor-critic network operating in continuous time, and with continuous state and action representations. In our model, the critic learns to predict expected future rewards in real time. Its activity, together with actual rewards, conditions the delivery of a neuromodulatory TD signal to itself and to the actor, which is responsible for action choice. In simulations, we show that such an architecture can solve a Morris water-maze-like navigation task, in a number of trials consistent with reported animal performance. We also use our model to solve the acrobot and the cartpole problems, two complex motor control tasks. Our model provides a plausible way of computing reward prediction error in the brain. Moreover, the analytically derived learning rule is consistent with experimental evidence for dopamine-modulated spike-timing-dependent plasticity.},
author = {Fr\'{e}maux, Nicolas and Sprekeler, Henning and Gerstner, Wulfram},
doi = {10.1371/journal.pcbi.1003024},
file = {:home/mzimmer/articles/journal.pcbi.1003024.pdf:pdf},
issn = {1553734X},
journal = {PLoS Computational Biology},
keywords = {modelling,reward,stdp,td},
pages = {e1003024+},
pmid = {23592970},
title = {{Reinforcement Learning Using a Continuous Time Actor-Critic Framework with Spiking Neurons}},
url = {http://dx.doi.org/10.1371/journal.pcbi.1003024},
volume = {9},
year = {2013}
}
@article{Doya,
author = {Doya, K},
journal = {Neural computation},
title = {{Reinforcement learning in continuous time and space}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976600300015961},
year = {2000}
}
@misc{Gibson1986,
abstract = {Gibson, J.J. (1979). The Ecological Approach to Visual Perception. Boston: Houghton Mifflin.ISBN 0898599598 (1986)},
author = {Gibson, J J},
booktitle = {Chapter Eight The Theory of Affordances},
pages = {127--136},
title = {{Gibson Theory of Affordances.pdf}},
year = {1986}
}
@article{Prokhorov1997,
abstract = {We discuss a variety of adaptive critic designs (ACDs) for neurocontrol. These are suitable for learning in noisy, nonlinear, and nonstationary environments. They have common roots as generalizations of dynamic programming for neural reinforcement learning approaches. Our discussion of these origins leads to an explanation of three design families: heuristic dynamic programming, dual heuristic programming, and globalized dual heuristic programming (GDHP). The main emphasis is on DHP and GDHP as advanced ACDs. We suggest two new modifications of the original GDHP design that are currently the only working implementations of GDHP. They promise to be useful for many engineering applications in the areas of optimization and optimal control. Based on one of these modifications, we present a unified approach to all ACDs. This leads to a generalized training procedure for ACDs.},
author = {Prokhorov, Danil V. and Wunsch, Donald C.},
doi = {10.1109/72.623201},
file = {:home/mzimmer/cours/Doctorat/Papers/00623201.pdf:pdf},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Adaptive Critic Design (ACD),Backpropagation,Control,DHP,Dynamic programming,GDHP,HDP,Heuristic dynamic programming,Neural network,Neurocontrol,Reinforcement learning},
number = {5},
pages = {997--1007},
pmid = {18255702},
title = {{Adaptive critic designs}},
volume = {8},
year = {1997}
}
@article{Bongard2010a,
abstract = {Embodied artificial intelligence argues that the body and brain play equally important roles in the generation of adaptive behavior. An increasingly common approach therefore is to evolve an agent's morphology along with its control in the hope that evolution will find a good coupled system. In order for embodied artificial intelligence to gain credibility within the robotics and cognitive science communities, however, it is necessary to amass evidence not only for how to co-optimize morphology and control of adaptive machines, but why. This work provides two new lines of evidence for why this co-optimization is useful: Here we show that for an object manipulation task in which a simulated robot must accomplish one, two, or three objectives simultaneously, subjugating more aspects of the robot's morphology to selective pressure allows for the evolution of better robots as the number of objectives increases. In addition, for robots that successfully evolved to accomplish all of their objectives, those composed of evolved rather than fixed morphologies generalized better to previously unseen environmental conditions.},
author = {Bongard, Josh},
doi = {10.1162/artl.2010.Bongard.024},
isbn = {1064-5462},
issn = {1064-5462},
journal = {Artificial life},
number = {3},
pages = {201--223},
pmid = {20059328},
title = {{The utility of evolving simulated robot morphology increases with task complexity for object manipulation.}},
volume = {16},
year = {2010}
}
@article{Peters2008a,
abstract = {In this paper, we suggest a novel reinforcement learning architecture, the Natural Actor-Critic. The actor updates are achieved using stochastic policy gradients employing Amari's natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke's Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Peters, Jan and Schaal, Stefan},
doi = {10.1016/j.neucom.2007.11.026},
isbn = {3540292438},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Actor-Critic methods,Compatible function approximation,Natural gradients,Policy-gradient methods,Reinforcement learning,Robot learning},
number = {7-9},
pages = {1180--1190},
title = {{Natural Actor-Critic}},
volume = {71},
year = {2008}
}
@article{Schaal2007,
abstract = {In the past, computational motor control has been approached from at least two major frameworks: the dynamic systems approach and the viewpoint of optimal control. The dynamic system approach emphasizes motor control as a process of self-organization between an animal and its environment. Nonlinear differential equations that can model entrainment and synchronization behavior are among the most favorable tools of dynamic systems modelers. In contrast, optimal control approaches view motor control as the evolutionary or development result of a nervous system that tries to optimize rather general organizational principles, e.g., energy consumption or accurate task achievement. Optimal control theory is usually employed to develop appropriate theories. Interestingly, there is rather little interaction between dynamic systems and optimal control modelers as the two approaches follow rather different philosophies and are often viewed as diametrically opposing. In this paper, we develop a computational approach to motor control that offers a unifying modeling framework for both dynamic systems and optimal control approaches. In discussions of several behavioral experiments and some theoretical and robotics studies, we demonstrate how our computational ideas allow both the representation of self-organizing processes and the optimization of movement based on reward criteria. Our modeling framework is rather simple and general, and opens opportunities to revisit many previous modeling results from this novel unifying view.},
author = {Schaal, Stefan and Mohajerian, Peyman and Ijspeert, Auke},
doi = {10.1016/S0079-6123(06)65027-9},
file = {:home/mzimmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaal, Mohajerian, Ijspeert - 2007 - Dynamics systems vs. optimal control--a unifying view.pdf:pdf},
issn = {0079-6123},
journal = {Progress in brain research},
keywords = {Animals,Computer Simulation,Environment,Humans,Models, Biological,Motor Activity,Motor Activity: physiology,Nonlinear Dynamics,Reward},
month = jan,
number = {1},
pages = {425--45},
pmid = {17925262},
title = {{Dynamics systems vs. optimal control--a unifying view.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17925262},
volume = {165},
year = {2007}
}
@article{Murray,
author = {Murray, Walter and Wright, Margaret H.},
doi = {10.1137/0804013},
journal = {SIAM Journal on Optimization},
pages = {229--246},
title = {{Line Search Procedures for the Logarithmic Barrier Function}},
volume = {4},
year = {1994}
}
@techreport{Riedmiller1994,
abstract = {Rprop stands for 'Resilient backpropagation' and is a local adaptive learning scheme, performing supervised batch learning in multi-layer perceptrons. For a detailed discussion see also. The basic principle of Rprop is to eliminate the harmful influence of the size of the partial derivative on the weight step.},
author = {Riedmiller, Martin},
booktitle = {Report},
pages = {5--6},
title = {{Rprop-description and implementation details}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3428},
year = {1994}
}
@article{Stulp2013,
author = {Stulp, Freek and Sigaud, Olivier},
doi = {10.2478/pjbr-2013-0003},
file = {:home/mzimmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stulp, Sigaud - 2013 - Robot Skill Learning From Reinforcement Learning to Evolution Strategies.pdf:pdf},
journal = {Paladyn, Journal of Behavioral Robotics},
keywords = {black-box optimization,dynamic movement primitives,evolution strategies,reinforcement learning},
number = {1},
pages = {49--61},
title = {{Robot Skill Learning: From Reinforcement Learning to Evolution Strategies}},
url = {http://www.degruyter.com/view/j/pjbr.2013.4.issue-1/pjbr-2013-0003/pjbr-2013-0003.xml},
volume = {4},
year = {2013}
}
@inproceedings{VanHasselt2007,
abstract = {Quite some research has been done on reinforcement learning in continuous environments, but the research on problems where the actions can also be chosen from a continuous space is much more limited. We present a new class of algorithms named continuous actor critic learning automaton (CACLA) that can handle continuous states and actions. The resulting algorithm is straightforward to implement. An experimental comparison is made between this algorithm and other algorithms that can handle continuous action spaces. These experiments show that CACLA performs much better than the other algorithms, especially when it is combined with a Gaussian exploration method},
author = {{Van Hasselt}, Hado and Wiering, Marco A.},
booktitle = {Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning, ADPRL 2007},
doi = {10.1109/ADPRL.2007.368199},
isbn = {1424407060},
pages = {272--279},
title = {{Reinforcement learning in continuous action spaces}},
year = {2007}
}
@inproceedings{Riedmiller2005,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef- fective training of a Q-value function represented by amulti-layer percep- tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11564096\_32},
isbn = {3540292438},
issn = {03029743},
pages = {317--328},
title = {{Neural fitted Q iteration - First experiences with a data efficient neural Reinforcement Learning method}},
volume = {3720 LNAI},
year = {2005}
}
@article{Kober2010a,
abstract = {The acquisition and self-improvement of novel motor skills is among the most important problems in robotics. Motor primitives offer one of the most promising frameworks for the application of machine learning techniques in this context. Employing an improved form of the dynamic systems motor primitives originally introduced by Ijspeert et al. [2], we show how both discrete and rhythmic tasks can be learned using a concerted approach of both imitation and reinforcement learning. For doing so, we present both learning algorithms and representations targeted for the practical application in robotics. Furthermore, we show that it is possible to include a start-up phase in rhythmic primitives. We show that two new motor skills, i.e., Ball-in-a-Cup and Ball-Paddling, can be learned on a real Barrett WAM robot arm at a pace similar to human learning while achieving a significantly more reliable final performance.},
author = {Kober, Jens and Peters, Jan},
doi = {10.1007/s10994-010-5223-6},
file = {:home/mzimmer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kober, Peters - 2009 - Policy search for motor primitives in robotics.pdf:pdf},
isbn = {978-1-4244-2788-8},
issn = {08856125},
journal = {Machine Learning},
keywords = {Episodic reinforcement learning,Motor control,Motor primitives,Policy learning},
month = nov,
number = {1-2},
pages = {171--203},
title = {{Policy search for motor primitives in robotics}},
url = {http://link.springer.com/10.1007/s10994-010-5223-6 http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics},
volume = {84},
year = {2011}
}
@article{Lin1992,
abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
author = {Lin, Long J.},
doi = {10.1007/BF00992699},
issn = {08856125},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,planning,teaching},
number = {3-4},
pages = {293--321},
title = {{Self-improving reactive agents based on reinforcement learning, planning and teaching}},
volume = {8},
year = {1992}
}
@inproceedings{VanHasselt2007a,
abstract = {Quite some research has been done on reinforcement learning in continuous environments, but the research on problems where the actions can also be chosen from a continuous space is much more limited. We present a new class of algorithms named continuous actor critic learning automaton (CACLA) that can handle continuous states and actions. The resulting algorithm is straightforward to implement. An experimental comparison is made between this algorithm and other algorithms that can handle continuous action spaces. These experiments show that CACLA performs much better than the other algorithms, especially when it is combined with a Gaussian exploration method},
author = {{Van Hasselt}, Hado and Wiering, Marco A.},
booktitle = {Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning, ADPRL 2007},
doi = {10.1109/ADPRL.2007.368199},
isbn = {1424407060},
pages = {272--279},
title = {{Reinforcement learning in continuous action spaces}},
year = {2007}
}
@article{Konda2003,
abstract = {In this paper, we propose and analyze a class of actor-critic algorithms. These ar two-time-scale algorithms in which the critic uses temporal difference (TD) learning with a linearly parameterized approximation architecture, and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should ideally span a subspace prescribed by the choice of parameterization of the actor. We study actorcritic algorithms for Markov decision processes with general state and action spaces. We state and prove two results regarding their convergence.},
author = {Konda, Vijay R and Tsitsiklis, John N},
doi = {10.1137/S0363012901385691},
issn = {0363-0129},
journal = {Control Optim},
number = {4},
pages = {1143--1166},
title = {{Actor-Critic Algorithms}},
volume = {42},
year = {2003}
}
